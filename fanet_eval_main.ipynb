{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "fanet_eval_main.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "OwoEgKvm3cjE",
        "v90XgWEKGLGk",
        "aGarf8rjjcZu",
        "C6GEMH4fgmgC"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goromal/FANet_Evaluation/blob/main/fanet_eval_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvVGRtJx3XgV"
      },
      "source": [
        "# FANet Evaluation - 6.862 Project\n",
        "\n",
        "\n",
        "\n",
        "*   David Elatov\n",
        "*   Dayne Howard\n",
        "*   Andrew Torgesen\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwoEgKvm3cjE"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Link Google Drive to Github repo, install needed software, access training and testing data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ajqG5KQ_kHn"
      },
      "source": [
        "### CUDA\n",
        "\n",
        "1. Go to **Menu > Runtime > Change runtime type** and make sure that GPU is enabled.\n",
        "2. Run the commands below to ensure that the GPU (and CUDA) is operational."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pZ2TKFFACfl",
        "outputId": "bc513046-c6b4-4031-ae06-3f9187667089"
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Apr  6 14:06:56 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEPCt2YqAZW5",
        "outputId": "c8d73437-59d5-4dcb-9d80-bf7d3577e348"
      },
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuEruD4f5sYI"
      },
      "source": [
        "### Repository\n",
        "\n",
        "Mount drive, clone repo, navigate to repo, and change working directory to access repo files. **Run ONCE per computing session.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGeFdZYKz05l",
        "outputId": "208946e1-3e62-4e0c-86af-e0b21c5e417b"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd gdrive/MyDrive/\n",
        "if not os.path.exists('FANet_Evaluation'):\n",
        "  print('Repo not present. Cloning...')\n",
        "  ! git clone https://github.com/goromal/FANet_Evaluation.git\n",
        "  %cd FANet_Evaluation/\n",
        "else:\n",
        "  print('Repo already present. Updating...')\n",
        "  %cd FANet_Evaluation/\n",
        "  ! git pull origin main\n",
        "from model.test_model import *\n",
        "test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive\n",
            "Repo already present. Updating...\n",
            "/content/gdrive/MyDrive/FANet_Evaluation\n",
            "From https://github.com/goromal/FANet_Evaluation\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Already up to date.\n",
            "SUCCESS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcYzRVcU5cqE"
      },
      "source": [
        "### Python Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGy-tLG03WtY",
        "outputId": "ecb1bfb8-0ef4-4669-ca2f-0edc3e50ebb9"
      },
      "source": [
        "! pip install oyaml\n",
        "! pip install torchstat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting oyaml\n",
            "  Downloading https://files.pythonhosted.org/packages/37/aa/111610d8bf5b1bb7a295a048fc648cec346347a8b0be5881defd2d1b4a52/oyaml-1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from oyaml) (3.13)\n",
            "Installing collected packages: oyaml\n",
            "Successfully installed oyaml-1.0\n",
            "Collecting torchstat\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/fe/f483b907ca80c90f189cd892bb2ce7b2c256010b30314bbec4fc17d1b5f1/torchstat-0.0.7-py3-none-any.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchstat) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchstat) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torchstat) (1.1.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchstat) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torchstat) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torchstat) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torchstat) (1.15.0)\n",
            "Installing collected packages: torchstat\n",
            "Successfully installed torchstat-0.0.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v90XgWEKGLGk"
      },
      "source": [
        "## FANet-18 Initial FPS Testing\n",
        "\n",
        "Ensure that FANet implementation can be loaded and accessed via the GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krFI5O1nGRnM"
      },
      "source": [
        "import torch\n",
        "import oyaml as yaml\n",
        "from torchstat import stat\n",
        "import time,os\n",
        "\n",
        "from model.fanet import FANet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VVvvTik_Mk-",
        "outputId": "c2d59973-7774-4e93-e42d-a6f6e0812171"
      },
      "source": [
        "network = FANet(backbone='resnet18')\n",
        "network.cuda()\n",
        "network.eval()\n",
        "t_cnt = 0.0\n",
        "with torch.no_grad():\n",
        "  input = torch.rand((1,3,1024,2048)).cuda()\n",
        "  \n",
        "  torch.cuda.synchronize()\n",
        "  x = network(input)\n",
        "  x = network(input)\n",
        "  \n",
        "  torch.cuda.synchronize()\n",
        "  torch.cuda.synchronize()\n",
        "  start_ts = time.time()\n",
        "\n",
        "  for i in range(100):\n",
        "    x = network(input)\n",
        "  \n",
        "  torch.cuda.synchronize()\n",
        "  end_ts = time.time()\n",
        "\n",
        "  t_cnt = end_ts-start_ts\n",
        "\n",
        "print('FANet-18 Performance (FPS): %f' % (100.0/t_cnt))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FANet-18 Performance (FPS): 40.201780\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGarf8rjjcZu"
      },
      "source": [
        "# Evaluation Pipeline Training Decomposition\n",
        "\n",
        "This is how the evaluation pipeline trains a model on CityScapes, from start to finish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3P-rxfRFjeUP"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/gdrive/MyDrive/FANet_Evaluation/evaluation') # so that the evaluation pipeline's internal imports work\n",
        "import tensorflow as tf\n",
        "from evaluation.utils.params import get_params\n",
        "from evaluation.utils.dirs import create_exp_dirs\n",
        "from evaluation.utils.misc import timeit\n",
        "import scipy.misc as misc # for image resizing\n",
        "from tqdm import tqdm # progress bar visualization\n",
        "import time # for timing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1WgfBS9lLnp"
      },
      "source": [
        "# Usable Models\n",
        "from evaluation.models.dilation_mobilenet import DilationMobileNet # << using this network as an example\n",
        "# etc...there's like 15 of them\n",
        "\n",
        "# Metrics for measuring performance (mIoU, etc.)\n",
        "from evaluation.metrics.metrics import Metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pDFhu8uASdc"
      },
      "source": [
        "# Argument class to instantiate a model\n",
        "class ModelTrainArgs(object):\n",
        "    def __init__(self):\n",
        "        # MODEL ARGS\n",
        "        self.img_width = 1024\n",
        "        self.img_height = 512\n",
        "        self.num_channels = 3 # 3 channels for color images\n",
        "        # data dir contains pre-processed weights.npy, X_train.npy, Y_train.npy, X_val.npy, Y_val.npy\n",
        "        self.data_dir = '/content/gdrive/MyDrive/full_cityscapes_res' # DATA LOCATED IN \"My Drive/full_cityscapes_res\"\n",
        "        self.weighted_loss = True\n",
        "        self.batch_size = 4\n",
        "        self.learning_rate = 0.0001\n",
        "        \n",
        "        # TRAIN ARGS\n",
        "        self.data_mode = \"experiment\"\n",
        "        self.num_classes = 20 # for CityScapes\n",
        "        self.test_every = 10 # validation performed every 10 training epochs\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hBjXsj3ASdd"
      },
      "source": [
        "# Set parameters\n",
        "args = ModelTrainArgs()\n",
        "\n",
        "# Reset the graph\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Create the sess\n",
        "gpu_options = tf.GPUOptions(allow_growth=True)\n",
        "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True))\n",
        "\n",
        "# Create Model class and \"build\" it\n",
        "with sess.as_default():\n",
        "    with tf.variable_scope('network') as scope:\n",
        "        model = DilationMobileNet(args)\n",
        "        model.build()\n",
        "\n",
        "# Instantiate training components\n",
        "sess.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))\n",
        "train_data = None\n",
        "train_data_len = None\n",
        "val_data = None\n",
        "val_data_len = None\n",
        "num_iterations_training_per_epoch = None\n",
        "num_iterations_validation_per_epoch = None\n",
        "metrics = Metrics(args.num_classes)\n",
        "\n",
        "# Training batch generator\n",
        "def train_generator():\n",
        "    global args, num_iterations_training_per_epoch, train_data_len, train_data\n",
        "    start = 0\n",
        "    idx = np.random.choice(train_data_len, num_iterations_training_per_epoch * args.batch_size, replace=True)\n",
        "    while True:\n",
        "        # select the mini_batches\n",
        "        mask = idx[start:start + args.batch_size]\n",
        "        x_batch = train_data['X'][mask]\n",
        "        y_batch = train_data['Y'][mask]\n",
        "\n",
        "        # update start idx\n",
        "        start += args.batch_size\n",
        "\n",
        "        yield x_batch, y_batch\n",
        "\n",
        "        if start >= train_data_len:\n",
        "            return\n",
        "\n",
        "# Load training and validation data\n",
        "print(\"Loading Training data..\")\n",
        "train_data_wrongsize = {'X': np.load(args.data_dir + \"X_train.npy\"), 'Y': np.load(args.data_dir + \"Y_train.npy\")}\n",
        "X = []\n",
        "Y = []\n",
        "train_data = dict()\n",
        "for i in range(train_data_wrongsize['X'].shape[0]):\n",
        "    X.append(misc.imresize(train_data_wrongsize['X'][i, ...], (args.img_height, args.img_width)))\n",
        "    Y.append(misc.imresize(train_data_wrongsize['Y'][i, ...], (args.img_height, args.img_width), 'nearest'))\n",
        "train_data['X'] = np.asarray(X)\n",
        "train_data['Y'] = np.asarray(Y)\n",
        "train_data_len = train_data['X'].shape[0]\n",
        "num_iterations_training_per_epoch = (train_data_len + args.batch_size - 1) // args.batch_size\n",
        "print(\"Train-shape-x -- \" + str(train_data['X'].shape) + \" \" + str(train_data_len))\n",
        "print(\"Train-shape-y -- \" + str(train_data['Y'].shape))\n",
        "print(\"Num of iterations on training data in one epoch -- \" + str(num_iterations_training_per_epoch))\n",
        "print(\"Training data is loaded\")\n",
        "\n",
        "print(\"Loading Validation data..\")\n",
        "val_data = {'X': np.load(args.data_dir + \"X_val.npy\"), 'Y': np.load(args.data_dir + \"Y_val.npy\")}\n",
        "val_data['Y_large'] = self.val_data['Y']\n",
        "val_data_len = val_data['X'].shape[0] - val_data['X'].shape[0] % args.batch_size\n",
        "num_iterations_validation_per_epoch = (val_data_len + args.batch_size - 1) // args.batch_size\n",
        "print(\"Val-shape-x -- \" + str(val_data['X'].shape) + \" \" + str(val_data_len))\n",
        "print(\"Val-shape-y -- \" + str(val_data['Y'].shape))\n",
        "print(\"Num of iterations on validation data in one epoch -- \" + str(num_iterations_validation_per_epoch))\n",
        "print(\"Validation data is loaded\")\n",
        "\n",
        "# Train\n",
        "print(\"Training mode will begin NOW ..\")\n",
        "for cur_epoch in range(self.model.global_epoch_tensor.eval(self.sess) + 1, self.args.num_epochs + 1, 1):\n",
        "\n",
        "    # init tqdm and get the epoch value\n",
        "    tt = tqdm(train_generator(), total=num_iterations_training_per_epoch, desc=\"epoch-\" + str(cur_epoch) + \"-\")\n",
        "\n",
        "    # init the current iterations\n",
        "    cur_iteration = 0\n",
        "\n",
        "    # init acc and loss lists\n",
        "    loss_list = []\n",
        "    acc_list = []\n",
        "\n",
        "    # loop by the number of iterations\n",
        "    for x_batch, y_batch in tt:\n",
        "\n",
        "        # get the cur_it for the summary\n",
        "        cur_it = model.global_step_tensor.eval(sess)\n",
        "\n",
        "        # Feed data into the network\n",
        "        feed_dict = {model.x_pl: x_batch,\n",
        "                     model.y_pl: y_batch,\n",
        "                     model.is_training: True}\n",
        "\n",
        "        # Run the feed forward but the last iteration finalize what you want to do\n",
        "        if cur_iteration < num_iterations_training_per_epoch - 1:\n",
        "\n",
        "            # run the feed_forward\n",
        "            _, loss, acc, summaries_merged = sess.run(\n",
        "                        [model.train_op, model.loss, model.accuracy, model.merged_summaries],\n",
        "                        feed_dict=feed_dict)\n",
        "            # log loss and acc\n",
        "            loss_list += [loss]\n",
        "            acc_list += [acc]\n",
        "\n",
        "        else:\n",
        "            # run the feed_forward\n",
        "            _, loss, acc, summaries_merged, segmented_imgs = self.sess.run(\n",
        "                            [model.train_op, model.loss, model.accuracy,\n",
        "                             model.merged_summaries, model.segmented_summary],\n",
        "                             feed_dict=feed_dict)\n",
        "\n",
        "            # log loss and acc\n",
        "            loss_list += [loss]\n",
        "            acc_list += [acc]\n",
        "            total_loss = np.mean(loss_list)\n",
        "            total_acc = np.mean(acc_list)\n",
        "\n",
        "            # Update the Global step\n",
        "            model.global_step_assign_op.eval(session=sess, feed_dict={model.global_step_input: cur_it + 1})\n",
        "\n",
        "            # Update the Cur Epoch tensor\n",
        "            # it is the last thing because if it is interrupted it repeat this\n",
        "            model.global_epoch_assign_op.eval(session=sess, feed_dict={model.global_epoch_input: cur_epoch + 1})\n",
        "\n",
        "            # print in console\n",
        "            tt.close()\n",
        "            print(\"epoch-\" + str(cur_epoch) + \"-\" + \"loss:\" + str(total_loss) + \"-\" + \" acc:\" + str(total_acc)[:6])\n",
        "\n",
        "            # Break the loop to finalize this epoch\n",
        "            break\n",
        "\n",
        "        # Update the Global step\n",
        "        model.global_step_assign_op.eval(session=sess, feed_dict={model.global_step_input: cur_it + 1})\n",
        "\n",
        "        # update the cur_iteration\n",
        "        cur_iteration += 1\n",
        "\n",
        "    # Test the model on validation set\n",
        "    if cur_epoch % args.test_every == 0:\n",
        "        step = model.global_step_tensor.eval(sess)\n",
        "        epoch = model.global_epoch_tensor.eval(sess)\n",
        "        print(\"Validation at step:\" + str(step) + \" at epoch:\" + str(epoch) + \" ..\")\n",
        "\n",
        "        # init tqdm and get the epoch value\n",
        "        tt = tqdm(range(num_iterations_validation_per_epoch), total=num_iterations_validation_per_epoch,\n",
        "                  desc=\"Val-epoch-\" + str(epoch) + \"-\")\n",
        "\n",
        "        # init acc and loss lists\n",
        "        loss_list = []\n",
        "        acc_list = []\n",
        "        inf_list = []\n",
        "\n",
        "        # idx of minibatch\n",
        "        idx = 0\n",
        "\n",
        "        # reset metrics\n",
        "        metrics.reset()\n",
        "\n",
        "        # get the maximum iou to compare with and save the best model\n",
        "        max_iou = model.best_iou_tensor.eval(self.sess)\n",
        "\n",
        "        # loop by the number of iterations\n",
        "        for cur_iteration in tt:\n",
        "            # load minibatches\n",
        "            x_batch = val_data['X'][idx:idx + args.batch_size]\n",
        "            y_batch = val_data['Y'][idx:idx + args.batch_size]\n",
        "\n",
        "            # update idx of minibatch\n",
        "            idx += args.batch_size\n",
        "\n",
        "            # Feed this variables to the network\n",
        "            feed_dict = {model.x_pl: x_batch,\n",
        "                         model.y_pl: y_batch,\n",
        "                         model.is_training: False}\n",
        "\n",
        "            # Run the feed forward but the last iteration finalize what you want to do\n",
        "            if cur_iteration < num_iterations_validation_per_epoch - 1:\n",
        "\n",
        "                start = time.time()\n",
        "                # run the feed_forward\n",
        "\n",
        "                out_argmax, loss, acc, summaries_merged = sess.run(\n",
        "                    [model.out_argmax, model.loss, model.accuracy, model.merged_summaries],\n",
        "                    feed_dict=feed_dict)\n",
        "\n",
        "                end = time.time()\n",
        "                # log loss and acc\n",
        "                loss_list += [loss]\n",
        "                acc_list += [acc]\n",
        "                inf_list += [end - start]\n",
        "\n",
        "                # log metrics\n",
        "                metrics.update_metrics_batch(out_argmax, y_batch)\n",
        "\n",
        "            else:\n",
        "                start = time.time()\n",
        "                # run the feed_forward\n",
        "                out_argmax, acc, segmented_imgs = sess.run(\n",
        "                        [test_model.out_argmax, test_model.accuracy, test_model.segmented_summary],\n",
        "                        feed_dict=feed_dict)\n",
        "\n",
        "                end = time.time()\n",
        "                # log loss and acc\n",
        "                acc_list += [acc]\n",
        "                inf_list += [end - start]\n",
        "                # log metrics\n",
        "                metrics.update_metrics_batch(out_argmax, y_batch)\n",
        "                # mean over batches\n",
        "                total_acc = np.mean(acc_list)\n",
        "                mean_iou = metrics.compute_final_metrics(num_iterations_validation_per_epoch)\n",
        "                mean_iou_arr = metrics.iou\n",
        "                mean_inference = str(np.mean(inf_list)) + '-seconds'\n",
        "\n",
        "                # print in console\n",
        "                tt.close()\n",
        "                print(\"Val-epoch-\" + str(epoch) + \"-\" +\n",
        "                      \"acc:\" + str(total_acc)[:6] + \"-mean_iou:\" + str(mean_iou))\n",
        "                print(\"Last_max_iou: \" + str(max_iou))\n",
        "                if mean_iou > max_iou:\n",
        "                    print(\"This validation got a new best iou. so we will save this one\")\n",
        "                    # Set the new maximum\n",
        "                    model.best_iou_assign_op.eval(session=sess, feed_dict={model.best_iou_input: mean_iou})\n",
        "                else:\n",
        "                    print(\"Hmm, not the best validation epoch :/..\")\n",
        "                break\n",
        "\n",
        "                # Break the loop to finalize this epoch\n",
        "        \n",
        "\n",
        "# Finish session\n",
        "self.sess.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6GEMH4fgmgC"
      },
      "source": [
        "# FANet Testing (Single-Frame Version)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG6dsWdYgr6a"
      },
      "source": [
        "# Dayne Howard, David Elatov, Andrew Torgesen, MIT, 6.862, Spring 2021\n",
        "# Credit for much of this code goes to MIT, \n",
        "# It was developed by staff for student use in course 6.036/6.862\n",
        "\n",
        "# We'll use the PyTorch Framework\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from torchstat import stat\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '/content/gdrive/MyDrive/FANet_Evaluation/evaluation') # so that the evaluation pipeline's internal imports work\n",
        "import oyaml as yaml\n",
        "from PIL import Image # for image resizing\n",
        "\n",
        "# Metrics for measuring performance (mIoU, etc.)\n",
        "from evaluation.metrics.metrics import Metrics\n",
        "\n",
        "# For displaying images later\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set a random seed for predictable behavior\n",
        "torch.manual_seed(6036)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahAu5IsRYAcJ"
      },
      "source": [
        "# We'll create a Dataset class to use with PyTorch's Built-In Dataloaders\n",
        "class FANetDataset(Dataset):\n",
        "    '''\n",
        "    A custom dataset class to use with PyTorch's built-in dataloaders.\n",
        "    This will make feeding images to our models much easier downstream.\n",
        "\n",
        "    data: np.arrays downloaded from Keras' databases\n",
        "    vectorize: if True, outputed image data will be (X,)\n",
        "                   if False, outputed image data will be (H,W)\n",
        "    '''\n",
        "    def __init__(self, data, labels, vectorize=False):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.vectorize = vectorize\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        image_data = self.data[idx,:]\n",
        "# # TO DO: Put in the Channels, height and width of the images used\n",
        "# \t\t    C = 3\n",
        "#         H = 512\n",
        "#         W = 1028\n",
        "\n",
        "#         image_data = image_data.reshape((C,H,W))\n",
        "#         if self.vectorize:\n",
        "#             image_data = image_data.reshape((H*W,))\n",
        "################ The line below might need [idx,:] or something\n",
        "        image_label = self.labels[idx]\n",
        "#########################################################################\n",
        "        return image_data, image_label\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "\n",
        "# Argument class to instantiate a model\n",
        "class ModelTrainArgs(object):\n",
        "    def __init__(self):\n",
        "        # MODEL ARGS\n",
        "        self.img_width = 1024\n",
        "        self.img_height = 512\n",
        "        self.num_channels = 3 # 3 channels for color images\n",
        "        # data dir contains pre-processed weights.npy, X_train.npy, Y_train.npy, X_val.npy, Y_val.npy\n",
        "        self.data_dir = '/content/gdrive/MyDrive/full_cityscapes_res/' # DATA LOCATED IN \"My Drive/full_cityscapes_res\"\n",
        "        self.weighted_loss = True\n",
        "        self.batch_size = 10\n",
        "        self.learning_rate = 0.0001\n",
        "        \n",
        "        # TRAIN ARGS\n",
        "        self.data_mode = \"experiment\"\n",
        "        self.num_classes = 20 # for CityScapes\n",
        "        self.test_every = 10 # validation performed every 10 training epochs\n",
        "\n",
        "# Set parameters\n",
        "args = ModelTrainArgs()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f66HjejM7szJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14d51219-c8ce-4d5a-aca5-b812850b65a0"
      },
      "source": [
        "########################## TO DO: Download Data Here\n",
        "#Split into training, validation, and testing data\n",
        "#train_data\n",
        "#val_data\n",
        "#test_data\n",
        "#EXAMPLE:\n",
        "# # Download MNIST Data\n",
        "# (mnist_train, _), (mnist_test, _) = mnist.load_data()\n",
        "\n",
        "# # Load data as Numpy arrays of size (#datapoints, 28*28=784)\n",
        "# mnist_train = mnist_train.astype('float32') / 255.\n",
        "# mnist_test = mnist_test.astype('float32') / 255.\n",
        "# mnist_train = mnist_train.reshape((len(mnist_train), np.prod(mnist_train.shape[1:])))\n",
        "# mnist_test = mnist_test.reshape((len(mnist_test), np.prod(mnist_test.shape[1:])))\n",
        "\n",
        "# # Split test data into a test and validation set:\n",
        "# val_data = mnist_test[:(mnist_test.shape[0]//2),:]\n",
        "# test_data = mnist_test[(mnist_test.shape[0]//2):,:]\n",
        "# train_data = mnist_train\n",
        "\n",
        "# Download training data. Resize the targets to match FANet output\n",
        "print(\"Loading Training data..\")\n",
        "###train_data_wrongsize = {'X': np.load(args.data_dir + \"x_train_small_set.npy\"), 'Y': np.load(args.data_dir + \"y_train_small_set.npy\")}\n",
        "###X = []\n",
        "Y = []\n",
        "X = np.load(args.data_dir + \"x_train_small_set.npy\")\n",
        "Y_wrongsize = np.load(args.data_dir + \"y_train_small_set.npy\")\n",
        "###train_data = dict()\n",
        "###train_labels = dict()\n",
        "for i in range(Y_wrongsize.shape[0]):\n",
        "    #On the next two lines, it looks like the dimensions are flipped, but that\n",
        "    #is because of how PIL.Image defines their images (width,height) instead\n",
        "    #of numpy's (rows,columns)\n",
        "    ###X.append(np.array(Image.fromarray(train_data_wrongsize['X'][i, ...]).resize((1024,512),Image.NEAREST)))\n",
        "    Y.append(np.array(Image.fromarray(Y_wrongsize[i, ...]).resize((128,64),Image.NEAREST)))\n",
        "\n",
        "train_data = (np.asarray(np.transpose(X,axes=(0,3,1,2)))/255.0).astype(np.float) #change to float and normalize RGB\n",
        "train_labels = np.asarray(Y)\n",
        "print(\"Done loading training data\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Training data..\n",
            "Done loading training data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOZufW27Xtsy",
        "outputId": "f6f46499-df93-4a5c-9525-295be9a6e679"
      },
      "source": [
        "# Download validation data. Resize the targets to match FANet output\n",
        "print(\"Loading Validation data..\")\n",
        "###val_data_wrongsize = {'X': np.load(args.data_dir + \"x_val_small_set.npy\"), 'Y': np.load(args.data_dir + \"y_val_small_set.npy\")}\n",
        "###X = []\n",
        "Y = []\n",
        "X = np.load(args.data_dir + \"x_val_small_set.npy\")\n",
        "Y_wrongsize = np.load(args.data_dir + \"y_val_small_set.npy\")\n",
        "###val_data = dict()\n",
        "###val_labels = dict()\n",
        "for i in range(Y_wrongsize.shape[0]):\n",
        "    ###X.append(np.array(Image.fromarray(val_data_wrongsize['X'][i, ...]).resize((1024,512),Image.NEAREST)))\n",
        "    ###Y.append(np.array(Image.fromarray(val_data_wrongsize['Y'][i, ...]).resize((128,64),Image.NEAREST)))\n",
        "    Y.append(np.array(Image.fromarray(Y_wrongsize[i, ...]).resize((128,64),Image.NEAREST)))\n",
        "\n",
        "val_data = (np.asarray(np.transpose(X,axes=(0,3,1,2)))/255.0).astype(np.float) #change to float and normalize RGB\n",
        "val_labels = np.asarray(Y)\n",
        "print(\"Done loading validation data\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Validation data..\n",
            "Done loading validation data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtQ0dpnxX3HQ",
        "outputId": "e9059ced-9832-488e-c333-834a3fc59777"
      },
      "source": [
        "# Display dataset information\n",
        "print(\"Downloaded the following data:\")\n",
        "print(f\"train_data has shape {train_data.shape}\")\n",
        "print(f\"train_labels has shape {train_labels.shape}\")\n",
        "print(f\"val_data has shape {val_data.shape}\")\n",
        "print(f\"val_labels has shape {val_labels.shape}\")\n",
        "\n",
        "# Create Dataset objects for each of our train/val/test sets\n",
        "train_dataset = FANetDataset(train_data, train_labels)\n",
        "val_dataset = FANetDataset(val_data, val_labels)\n",
        "#test_dataset = FANetDataset(test_data, test_labels)\n",
        "\n",
        "# Create a PyTorch dataloader for each train/val/test set\n",
        "# Batch Size: 16 was what FANet used in their paper\n",
        "train_loader = DataLoader(train_dataset, batch_size= args.batch_size)\n",
        "val_loader = DataLoader(val_dataset, batch_size= args.batch_size)\n",
        "#test_loader = DataLoader(test_dataset, batch_size= args.batch_size)\n",
        "\n",
        "# Display dataloader info\n",
        "print(\"Created the following Dataloaders:\")\n",
        "print(f\"train_loader has {len(train_loader)} batches of training data\")\n",
        "print(f\"val_loader has {len(val_loader)} batches of validation data\")\n",
        "#print(f\"test_loader has {len(test_loader)} batches of testing data\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloaded the following data:\n",
            "train_data has shape (200, 3, 512, 1024)\n",
            "train_labels has shape (200, 64, 128)\n",
            "val_data has shape (50, 3, 512, 1024)\n",
            "val_labels has shape (50, 64, 128)\n",
            "Created the following Dataloaders:\n",
            "train_loader has 20 batches of training data\n",
            "val_loader has 5 batches of validation data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkR-FQjblR3c"
      },
      "source": [
        "def to_one_hot(tensor,device,nClasses=20):\n",
        "    n,h,w = tensor.size()\n",
        "    one_hot = torch.zeros(n,nClasses,h,w).to(device).scatter_(1,tensor.view(n,1,h,w),1)\n",
        "    return one_hot\n",
        "\n",
        "class mIoULoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True, n_classes=20):\n",
        "        super(mIoULoss, self).__init__()\n",
        "        self.classes = n_classes\n",
        "\n",
        "    def forward(self, inputs, target_oneHot):\n",
        "    \t# inputs => N x Classes x H x W\n",
        "    \t# target_oneHot => N x Classes x H x W\n",
        "\n",
        "    \tN = inputs.size()[0]\n",
        "\n",
        "    \t# predicted probabilities for each pixel along channel\n",
        "    \tinputs = F.softmax(inputs,dim=1)\n",
        "    \t\n",
        "    \t# Numerator Product\n",
        "    \tinter = inputs * target_oneHot\n",
        "    \t## Sum over all pixels N x C x H x W => N x C\n",
        "    \tinter = inter.view(N,self.classes,-1).sum(2)\n",
        "\n",
        "    \t#Denominator \n",
        "    \tunion= inputs + target_oneHot - (inputs*target_oneHot)\n",
        "    \t## Sum over all pixels N x C x H x W => N x C\n",
        "    \tunion = union.view(N,self.classes,-1).sum(2)\n",
        "\n",
        "    \tloss = inter/union\n",
        "\n",
        "    \t## Return average loss over classes and batch\n",
        "    \treturn loss.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpDX7McM7dIR"
      },
      "source": [
        "def train(model, device, train_loader, optimizer, val_loader=None):\n",
        "    '''\n",
        "    Function for training our networks. One call to train() performs a single\n",
        "    epoch for training.\n",
        "    model: an instance of our model\n",
        "    device: either \"cpu\" or \"cuda\", depending on if you're running with GPU support\n",
        "    train_loader: the dataloader for the training set\n",
        "    optimizer: optimizer used for training (the optimizer implements SGD)\n",
        "    val_loader: (optional) validation set to include \n",
        "    '''\n",
        "\n",
        "    # Set the model to training mode.\n",
        "    model.train()\n",
        "\n",
        "    #we'll keep adding the loss of each batch to total_loss, so we can calculate\n",
        "    #the average loss at the end of the epoch.\n",
        "    total_loss = 0\n",
        "    mIoU       = 0\n",
        "\n",
        "    # We'll iterate through each batch. One call of train() trains for 1 epoch.\n",
        "    # batch_idx: an integer representing which batch number we're on\n",
        "    # input: a pytorch tensor representing a batch of input images.\n",
        "    for batch_idx, (input,target) in enumerate(train_loader):\n",
        "        # This line sends data to GPU if you're using a GPU\n",
        "        input = input.to(device, dtype=torch.float)\n",
        "        target = target.type(torch.LongTensor).to(device)\n",
        "\n",
        "        # initialze the optimizer (the optimizer implements SGD)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # feed our input through the network\n",
        "        output = model.forward(input)\n",
        "        loss_function = nn.CrossEntropyLoss()\n",
        "        loss_value = loss_function(output,target)\n",
        "\n",
        "        # Perform backprop\n",
        "        loss_value.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        #accumulate loss to later calculate the average\n",
        "        total_loss += loss_value\n",
        "\n",
        "        #Calculate the mIoU\n",
        "        mIoU_function = mIoULoss()\n",
        "        target_onehot = to_one_hot(target, device)\n",
        "        mIoU += mIoU_function(output, target_onehot)\n",
        "\n",
        "    return (total_loss.item()/len(train_loader), mIoU.item()/len(train_loader))\n",
        "\n",
        "def test(model, device, val_loader):\n",
        "    '''\n",
        "    Function for testing our models. One call to test() runs through every\n",
        "    datapoint in our dataset once.\n",
        "    model: an instance of our model\n",
        "    device: either \"cpu\" or \"cuda:0\", depending on if you're running with GPU support\n",
        "    val_loader: the dataloader for the data to run the model on\n",
        "    '''\n",
        "    # set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # we'll keep track of total loss to calculate the average later\n",
        "    test_loss = 0\n",
        "    mIoU      = 0\n",
        "\n",
        "    #don't perform backprop if testing\n",
        "    with torch.no_grad():\n",
        "        # iterate thorugh each test image\n",
        "        for (input,target) in val_loader:\n",
        "\n",
        "            # send input image to GPU if using GPU\n",
        "            input = input.to(device, dtype=torch.float)\n",
        "            target = target.type(torch.LongTensor).to(device)\n",
        "\n",
        "            # run input through our model\n",
        "            output = model(input)\n",
        "\n",
        "            loss_function = nn.CrossEntropyLoss() \n",
        "            loss_value = loss_function(output,target) \n",
        "            test_loss += loss_value\n",
        "\n",
        "            #Calculate the mIoU\n",
        "            mIoU_function = mIoULoss()\n",
        "            target_onehot = to_one_hot(target, device)\n",
        "            mIoU += mIoU_function(output, target_onehot)\n",
        "\n",
        "    return (test_loss.item()/len(val_loader), mIoU.item()/len(val_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA3JLqV38kEq"
      },
      "source": [
        "#####################  FANet Model  ######################################\n",
        "class BatchNorm2d(nn.BatchNorm2d):\n",
        "    '''(conv => BN => ReLU) * 2'''\n",
        "    def __init__(self, num_features, activation='none'):\n",
        "        super(BatchNorm2d, self).__init__(num_features=num_features)\n",
        "        if activation == 'leaky_relu':\n",
        "            self.activation = nn.LeakyReLU()\n",
        "        elif activation == 'none':\n",
        "            self.activation = lambda x:x\n",
        "        else:\n",
        "            raise Exception(\"Accepted activation: ['leaky_relu']\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.activation(x)\n",
        "\n",
        "up_kwargs = {'mode': 'bilinear', 'align_corners': True}\n",
        "\n",
        "\n",
        "class FANet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 nclass=20,\n",
        "                 backbone='resnet18',\n",
        "                 norm_layer=BatchNorm2d):\n",
        "        super(FANet, self).__init__()\n",
        "\n",
        "        self.norm_layer = norm_layer\n",
        "        self._up_kwargs = up_kwargs\n",
        "        self.nclass = nclass\n",
        "        self.backbone = backbone\n",
        "        if backbone == 'resnet18':\n",
        "            self.expansion = 1\n",
        "            self.resnet = Resnet18(norm_layer=norm_layer)\n",
        "        elif backbone == 'resnet34':\n",
        "            self.expansion = 1\n",
        "            self.resnet = Resnet34(norm_layer=norm_layer)\n",
        "        elif backbone == 'resnet50':\n",
        "            self.expansion = 4\n",
        "            self.resnet = Resnet50(norm_layer=norm_layer)\n",
        "        elif backbone == 'resnet101':\n",
        "            self.expansion = 4\n",
        "            self.resnet = Resnet101(norm_layer=norm_layer)\n",
        "        elif backbone == 'resnet152':\n",
        "            self.expansion = 4\n",
        "            self.resnet = Resnet152(norm_layer=norm_layer)\n",
        "        else:\n",
        "            raise RuntimeError('unknown backbone: {}'.format(backbone))\n",
        "\n",
        "        self.fam_32 = FastAttModule(512*self.expansion,256,128,norm_layer=norm_layer)\n",
        "        self.fam_16 = FastAttModule(256*self.expansion,256,128,norm_layer=norm_layer)\n",
        "        self.fam_8 = FastAttModule(128*self.expansion,256,128,norm_layer=norm_layer)\n",
        "        self.fam_4 = FastAttModule(64*self.expansion,256,128,norm_layer=norm_layer)\n",
        "\n",
        "        self.clslayer  = FPNOutput(256, 256, nclass,norm_layer=norm_layer)\n",
        "\n",
        "    def forward(self, x, lbl=None):\n",
        "\n",
        "        _, _, h, w = x.size()\n",
        "\n",
        "        feat4, feat8, feat16, feat32 = self.resnet(x)\n",
        "\n",
        "        upfeat_32, smfeat_32 = self.fam_32(feat32,None,True,True)\n",
        "        upfeat_16, smfeat_16 = self.fam_16(feat16,upfeat_32,True,True)\n",
        "        upfeat_8 = self.fam_8(feat8,upfeat_16,True,False)\n",
        "        smfeat_4 = self.fam_4(feat4,upfeat_8,False,True)\n",
        "\n",
        "        x = self._upsample_cat(smfeat_16, smfeat_4)\n",
        "\n",
        "        outputs = self.clslayer(x)\n",
        "        \n",
        "        return outputs\n",
        "\n",
        "    def _upsample_cat(self, x1, x2):\n",
        "        '''Upsample and concatenate feature maps.\n",
        "        '''\n",
        "        _,_,H,W = x2.size()\n",
        "        x1 = F.interpolate(x1, (H,W), **self._up_kwargs)\n",
        "        x = torch.cat([x1,x2],dim=1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvBNReLU(nn.Module):\n",
        "    def __init__(self, in_chan, out_chan, ks=3, stride=1, padding=1, norm_layer=None, activation='leaky_relu',*args, **kwargs):\n",
        "        super(ConvBNReLU, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_chan,\n",
        "                out_chan,\n",
        "                kernel_size = ks,\n",
        "                stride = stride,\n",
        "                padding = padding,\n",
        "                bias = False)\n",
        "        self.norm_layer = norm_layer\n",
        "        if self.norm_layer is not None:\n",
        "            self.bn = norm_layer(out_chan, activation=activation)\n",
        "        else:\n",
        "            self.bn =  lambda x:x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FPNOutput(nn.Module):\n",
        "    def __init__(self, in_chan, mid_chan, n_classes, norm_layer=None, *args, **kwargs):\n",
        "        super(FPNOutput, self).__init__()\n",
        "        self.norm_layer = norm_layer\n",
        "        self.conv = ConvBNReLU(in_chan, mid_chan, ks=3, stride=1, padding=1, norm_layer=norm_layer)\n",
        "        self.conv_out = nn.Conv2d(mid_chan, n_classes, kernel_size=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.conv_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FastAttModule(nn.Module):\n",
        "    def __init__(self, in_chan, mid_chn=256, out_chan=128, norm_layer=None, *args, **kwargs):\n",
        "        super(FastAttModule, self).__init__()\n",
        "        self.norm_layer = norm_layer\n",
        "        self._up_kwargs = up_kwargs\n",
        "        mid_chn = int(in_chan/2)        \n",
        "        self.w_qs = ConvBNReLU(in_chan, 32, ks=1, stride=1, padding=0, norm_layer=norm_layer, activation='none')\n",
        "\n",
        "        self.w_ks = ConvBNReLU(in_chan, 32, ks=1, stride=1, padding=0, norm_layer=norm_layer, activation='none')\n",
        "\n",
        "        self.w_vs = ConvBNReLU(in_chan, in_chan, ks=1, stride=1, padding=0, norm_layer=norm_layer)\n",
        "\n",
        "        self.latlayer3 = ConvBNReLU(in_chan, in_chan, ks=1, stride=1, padding=0, norm_layer=norm_layer)\n",
        "\n",
        "        self.up = ConvBNReLU(in_chan, mid_chn, ks=1, stride=1, padding=1, norm_layer=norm_layer)\n",
        "        self.smooth = ConvBNReLU(in_chan, out_chan, ks=3, stride=1, padding=1, norm_layer=norm_layer)\n",
        "\n",
        "    def forward(self, feat, up_fea_in,up_flag, smf_flag):\n",
        "\n",
        "        query = self.w_qs(feat)\n",
        "        key   = self.w_ks(feat)\n",
        "        value = self.w_vs(feat)\n",
        "\n",
        "        N,C,H,W = feat.size()\n",
        "\n",
        "        query_ = query.view(N,32,-1).permute(0, 2, 1)\n",
        "        query = F.normalize(query_, p=2, dim=2, eps=1e-12)\n",
        "\n",
        "        key_   = key.view(N,32,-1)\n",
        "        key   = F.normalize(key_, p=2, dim=1, eps=1e-12)\n",
        "\n",
        "        value = value.view(N,C,-1).permute(0, 2, 1)\n",
        "\n",
        "        f = torch.matmul(key, value)\n",
        "        y = torch.matmul(query, f)\n",
        "        y = y.permute(0, 2, 1).contiguous()\n",
        "\n",
        "        y = y.view(N, C, H, W)\n",
        "        W_y = self.latlayer3(y)\n",
        "        p_feat = W_y + feat\n",
        "\n",
        "        if up_flag and smf_flag:\n",
        "            if up_fea_in is not None:\n",
        "                p_feat = self._upsample_add(up_fea_in, p_feat)\n",
        "            up_feat = self.up(p_feat)\n",
        "            smooth_feat = self.smooth(p_feat)\n",
        "            return up_feat, smooth_feat\n",
        "\n",
        "        if up_flag and not smf_flag:\n",
        "            if up_fea_in is not None:\n",
        "                p_feat = self._upsample_add(up_fea_in, p_feat)\n",
        "            up_feat = self.up(p_feat)\n",
        "            return up_feat\n",
        "\n",
        "        if not up_flag and smf_flag:\n",
        "            if up_fea_in is not None:\n",
        "                p_feat = self._upsample_add(up_fea_in, p_feat)\n",
        "            smooth_feat = self.smooth(p_feat)\n",
        "            return smooth_feat\n",
        "\n",
        "    def _upsample_add(self, x, y):\n",
        "        '''Upsample and add two feature maps.\n",
        "        '''\n",
        "        _,_,H,W = y.size()\n",
        "        return F.interpolate(x, (H,W), **self._up_kwargs) + y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6I9NvEea8qf"
      },
      "source": [
        "model_urls = {'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth'}\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
        "                     padding=0, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_chan, out_chan, stride=1, norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.norm_layer = norm_layer\n",
        "        self.conv1 = conv3x3(in_chan, out_chan, stride)\n",
        "        self.bn1 = norm_layer(out_chan, activation='leaky_relu')\n",
        "        self.conv2 = conv3x3(out_chan, out_chan)\n",
        "        self.bn2 = norm_layer(out_chan, activation='none')\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = None\n",
        "        if in_chan != out_chan or stride != 1:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_chan, out_chan,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                norm_layer(out_chan, activation='none'),\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        shortcut = x\n",
        "        if self.downsample is not None:\n",
        "            shortcut = self.downsample(x)\n",
        "\n",
        "        out_ = shortcut + out\n",
        "        out_ = self.relu(out_)\n",
        "        return out_\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    def __init__(self, in_chan, out_chan, stride=1, base_width=64, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        width = int(out_chan*(base_width / 64.)) * 1\n",
        "        self.norm_layer = norm_layer\n",
        "        self.conv1 = conv1x1(in_chan, width)\n",
        "        self.bn1 = norm_layer(width, activation='leaky_relu')\n",
        "        self.conv2 = conv3x3(width, width, stride)\n",
        "        self.bn2 = norm_layer(width, activation='leaky_relu')\n",
        "        self.conv3 = conv1x1(width, out_chan * self.expansion)\n",
        "        self.bn3 = norm_layer(out_chan * self.expansion, activation='none')\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = None\n",
        "        if in_chan != out_chan*self.expansion or stride != 1:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_chan, out_chan*self.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                norm_layer(out_chan*self.expansion, activation='none'),\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        shortcut = x\n",
        "        if self.downsample is not None:\n",
        "            shortcut = self.downsample(x)\n",
        "\n",
        "        out_ = shortcut +out\n",
        "        out_ = self.relu(out_)\n",
        "\n",
        "        return out_\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, strides, norm_layer=None):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.norm_layer = norm_layer\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = norm_layer(64, activation='leaky_relu')\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.inplanes = 64\n",
        "        self.layer1 = self.create_layer(block,   64, bnum=layers[0], stride=strides[0], norm_layer=norm_layer)\n",
        "        self.layer2 = self.create_layer(block,  128, bnum=layers[1], stride=strides[1], norm_layer=norm_layer)\n",
        "        self.layer3 = self.create_layer(block,  256, bnum=layers[2], stride=strides[2], norm_layer=norm_layer)\n",
        "        self.layer4 = self.create_layer(block,  512, bnum=layers[3], stride=strides[3], norm_layer=norm_layer)\n",
        "\n",
        "    def create_layer(self, block , out_chan, bnum, stride=1,norm_layer=None):\n",
        "        layers = [block(self.inplanes, out_chan, stride=stride, norm_layer=norm_layer)]\n",
        "        self.inplanes = out_chan*block.expansion\n",
        "        for i in range(bnum-1):\n",
        "            layers.append(block(self.inplanes, out_chan, stride=1, norm_layer=norm_layer))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        feat4 = self.layer1(x)\n",
        "        feat8 = self.layer2(feat4) # 1/8\n",
        "        feat16 = self.layer3(feat8) # 1/16\n",
        "        feat32 = self.layer4(feat16) # 1/32\n",
        "        return feat4, feat8, feat16, feat32\n",
        "\n",
        "    def init_weight(self,state_dict):\n",
        "        self_state_dict = self.state_dict()\n",
        "        for k, v in state_dict.items():\n",
        "            if 'fc' in k: continue\n",
        "            self_state_dict.update({k: v})\n",
        "        self.load_state_dict(self_state_dict, strict=True)\n",
        "\n",
        "def Resnet18(pretrained=True, norm_layer=None, **kwargs):\n",
        "    model = ResNet(BasicBlock, [2, 2, 2, 2],[2, 2, 2, 2], norm_layer=norm_layer)\n",
        "    if pretrained:\n",
        "        model.init_weight(model_zoo.load_url(model_urls['resnet18']))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa0pGtld8pDf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c7eea90-5747-4df2-f224-25256f06edd8"
      },
      "source": [
        "# Check if using CPU or GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# create an instance of our CNN\n",
        "network = FANet(backbone='resnet18').to(device)\n",
        "\n",
        "# initialize our optimizer. We'll use Adam\n",
        "optimizer = torch.optim.Adam(network.parameters())\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "# Train the CNN\n",
        "for epoch in range(1, epochs+1):\n",
        "    (train_loss, train_mIoU) = train(network, device, train_loader, optimizer)\n",
        "    (val_loss, val_mIoU) = test(network, device, val_loader)\n",
        "    print('Train Epoch: {:02d} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tTraining mIoU: {:.6f} \\tValidation mIoU: {:.6f}'.format(epoch, train_loss, val_loss, train_mIoU, val_mIoU))\n",
        "\n",
        "# Test the CNN\n",
        "#print('\\nTest Loss:', test(network, device, test_loader))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 01 \tTraining Loss: 5.193585 \tValidation Loss: 1.815554 \tTraining mIoU: 0.024720 \tValidation mIoU: 0.043057\n",
            "Train Epoch: 02 \tTraining Loss: 1.658912 \tValidation Loss: 1.495707 \tTraining mIoU: 0.054932 \tValidation mIoU: 0.077109\n",
            "Train Epoch: 03 \tTraining Loss: 1.303959 \tValidation Loss: 1.134543 \tTraining mIoU: 0.083167 \tValidation mIoU: 0.102064\n",
            "Train Epoch: 04 \tTraining Loss: 1.147002 \tValidation Loss: 1.114584 \tTraining mIoU: 0.104511 \tValidation mIoU: 0.111412\n",
            "Train Epoch: 05 \tTraining Loss: 1.081876 \tValidation Loss: 1.129767 \tTraining mIoU: 0.110686 \tValidation mIoU: 0.111982\n",
            "Train Epoch: 06 \tTraining Loss: 1.034072 \tValidation Loss: 1.026069 \tTraining mIoU: 0.118301 \tValidation mIoU: 0.120688\n",
            "Train Epoch: 07 \tTraining Loss: 1.006999 \tValidation Loss: 1.020619 \tTraining mIoU: 0.124740 \tValidation mIoU: 0.120670\n",
            "Train Epoch: 08 \tTraining Loss: 0.986638 \tValidation Loss: 0.939766 \tTraining mIoU: 0.126526 \tValidation mIoU: 0.134797\n",
            "Train Epoch: 09 \tTraining Loss: 0.937189 \tValidation Loss: 0.982630 \tTraining mIoU: 0.136320 \tValidation mIoU: 0.134456\n",
            "Train Epoch: 10 \tTraining Loss: 0.927685 \tValidation Loss: 0.948029 \tTraining mIoU: 0.138654 \tValidation mIoU: 0.136818\n",
            "Train Epoch: 11 \tTraining Loss: 0.905670 \tValidation Loss: 0.928623 \tTraining mIoU: 0.141493 \tValidation mIoU: 0.139078\n",
            "Train Epoch: 12 \tTraining Loss: 0.904704 \tValidation Loss: 0.941815 \tTraining mIoU: 0.144053 \tValidation mIoU: 0.134619\n",
            "Train Epoch: 13 \tTraining Loss: 0.870661 \tValidation Loss: 0.917990 \tTraining mIoU: 0.146838 \tValidation mIoU: 0.145513\n",
            "Train Epoch: 14 \tTraining Loss: 0.851453 \tValidation Loss: 0.907519 \tTraining mIoU: 0.150265 \tValidation mIoU: 0.143969\n",
            "Train Epoch: 15 \tTraining Loss: 0.827373 \tValidation Loss: 0.856597 \tTraining mIoU: 0.153947 \tValidation mIoU: 0.155860\n",
            "Train Epoch: 16 \tTraining Loss: 0.815213 \tValidation Loss: 0.895740 \tTraining mIoU: 0.155240 \tValidation mIoU: 0.164546\n",
            "Train Epoch: 17 \tTraining Loss: 0.819910 \tValidation Loss: 0.884004 \tTraining mIoU: 0.155735 \tValidation mIoU: 0.156568\n",
            "Train Epoch: 18 \tTraining Loss: 0.812510 \tValidation Loss: 0.914925 \tTraining mIoU: 0.158626 \tValidation mIoU: 0.142516\n",
            "Train Epoch: 19 \tTraining Loss: 0.780380 \tValidation Loss: 0.892484 \tTraining mIoU: 0.160910 \tValidation mIoU: 0.150155\n",
            "Train Epoch: 20 \tTraining Loss: 0.745891 \tValidation Loss: 0.952524 \tTraining mIoU: 0.168122 \tValidation mIoU: 0.157171\n",
            "Train Epoch: 21 \tTraining Loss: 0.738972 \tValidation Loss: 0.819076 \tTraining mIoU: 0.170062 \tValidation mIoU: 0.155876\n",
            "Train Epoch: 22 \tTraining Loss: 0.733406 \tValidation Loss: 0.814750 \tTraining mIoU: 0.171751 \tValidation mIoU: 0.158240\n",
            "Train Epoch: 23 \tTraining Loss: 0.724720 \tValidation Loss: 0.831282 \tTraining mIoU: 0.173595 \tValidation mIoU: 0.155955\n",
            "Train Epoch: 24 \tTraining Loss: 0.734009 \tValidation Loss: 0.886146 \tTraining mIoU: 0.171155 \tValidation mIoU: 0.156081\n",
            "Train Epoch: 25 \tTraining Loss: 0.750450 \tValidation Loss: 0.850909 \tTraining mIoU: 0.167117 \tValidation mIoU: 0.155202\n",
            "Train Epoch: 26 \tTraining Loss: 0.704243 \tValidation Loss: 0.854321 \tTraining mIoU: 0.176991 \tValidation mIoU: 0.167278\n",
            "Train Epoch: 27 \tTraining Loss: 0.700559 \tValidation Loss: 0.903273 \tTraining mIoU: 0.178189 \tValidation mIoU: 0.167615\n",
            "Train Epoch: 28 \tTraining Loss: 0.682985 \tValidation Loss: 0.818230 \tTraining mIoU: 0.181621 \tValidation mIoU: 0.163752\n",
            "Train Epoch: 29 \tTraining Loss: 0.662112 \tValidation Loss: 0.814621 \tTraining mIoU: 0.186929 \tValidation mIoU: 0.168390\n",
            "Train Epoch: 30 \tTraining Loss: 0.653168 \tValidation Loss: 0.964118 \tTraining mIoU: 0.188273 \tValidation mIoU: 0.169397\n",
            "Train Epoch: 31 \tTraining Loss: 0.659869 \tValidation Loss: 1.098916 \tTraining mIoU: 0.186925 \tValidation mIoU: 0.150930\n",
            "Train Epoch: 32 \tTraining Loss: 0.665921 \tValidation Loss: 0.941090 \tTraining mIoU: 0.185360 \tValidation mIoU: 0.157537\n",
            "Train Epoch: 33 \tTraining Loss: 0.622943 \tValidation Loss: 0.877039 \tTraining mIoU: 0.195632 \tValidation mIoU: 0.168195\n",
            "Train Epoch: 34 \tTraining Loss: 0.587959 \tValidation Loss: 0.875626 \tTraining mIoU: 0.201102 \tValidation mIoU: 0.165905\n",
            "Train Epoch: 35 \tTraining Loss: 0.585090 \tValidation Loss: 0.792256 \tTraining mIoU: 0.202840 \tValidation mIoU: 0.168466\n",
            "Train Epoch: 36 \tTraining Loss: 0.573054 \tValidation Loss: 0.779876 \tTraining mIoU: 0.205620 \tValidation mIoU: 0.174773\n",
            "Train Epoch: 37 \tTraining Loss: 0.548101 \tValidation Loss: 0.790334 \tTraining mIoU: 0.211415 \tValidation mIoU: 0.174248\n",
            "Train Epoch: 38 \tTraining Loss: 0.519060 \tValidation Loss: 0.858092 \tTraining mIoU: 0.217312 \tValidation mIoU: 0.172911\n",
            "Train Epoch: 39 \tTraining Loss: 0.498791 \tValidation Loss: 0.846351 \tTraining mIoU: 0.221995 \tValidation mIoU: 0.176184\n",
            "Train Epoch: 40 \tTraining Loss: 0.489123 \tValidation Loss: 0.805232 \tTraining mIoU: 0.224976 \tValidation mIoU: 0.183272\n",
            "Train Epoch: 41 \tTraining Loss: 0.490949 \tValidation Loss: 0.843541 \tTraining mIoU: 0.224439 \tValidation mIoU: 0.175092\n",
            "Train Epoch: 42 \tTraining Loss: 0.476466 \tValidation Loss: 0.874799 \tTraining mIoU: 0.227555 \tValidation mIoU: 0.171677\n",
            "Train Epoch: 43 \tTraining Loss: 0.465619 \tValidation Loss: 0.815851 \tTraining mIoU: 0.231299 \tValidation mIoU: 0.181123\n",
            "Train Epoch: 44 \tTraining Loss: 0.460007 \tValidation Loss: 0.855914 \tTraining mIoU: 0.234172 \tValidation mIoU: 0.182943\n",
            "Train Epoch: 45 \tTraining Loss: 0.460516 \tValidation Loss: 0.945925 \tTraining mIoU: 0.232362 \tValidation mIoU: 0.178285\n",
            "Train Epoch: 46 \tTraining Loss: 0.468692 \tValidation Loss: 0.824884 \tTraining mIoU: 0.230930 \tValidation mIoU: 0.186811\n",
            "Train Epoch: 47 \tTraining Loss: 0.466746 \tValidation Loss: 0.819449 \tTraining mIoU: 0.231332 \tValidation mIoU: 0.192913\n",
            "Train Epoch: 48 \tTraining Loss: 0.426578 \tValidation Loss: 0.873337 \tTraining mIoU: 0.241093 \tValidation mIoU: 0.184548\n",
            "Train Epoch: 49 \tTraining Loss: 0.412017 \tValidation Loss: 0.927230 \tTraining mIoU: 0.245945 \tValidation mIoU: 0.180238\n",
            "Train Epoch: 50 \tTraining Loss: 0.414595 \tValidation Loss: 0.941157 \tTraining mIoU: 0.248097 \tValidation mIoU: 0.176727\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}